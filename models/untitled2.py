# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XSYFLIOj1K2YHDj88vukfRfF8oqxcRiW
"""

!pip install tensorflow opencv-python seaborn


import os, zipfile, numpy as np, matplotlib.pyplot as plt, seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.utils.class_weight import compute_class_weight

from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.models import Model
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator


zip_path = '/content/dataset.zip'
extract_path = '/content/data'
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

paper_dir = os.path.join(extract_path, 'dataset', 'paper')
non_paper_dir = os.path.join(extract_path, 'dataset', 'non-paper')

# Step 4: Load and label data
image_size = 256
images, labels = [], []

for file in os.listdir(paper_dir):
    try:
        img = load_img(os.path.join(paper_dir, file), target_size=(image_size, image_size))
        img = img_to_array(img) / 255.0
        images.append(img)
        labels.append(1)
    except: continue

for file in os.listdir(non_paper_dir):
    try:
        img = load_img(os.path.join(non_paper_dir, file), target_size=(image_size, image_size))
        img = img_to_array(img) / 255.0
        images.append(img)
        labels.append(0)
    except: continue

X = np.array(images)
y = np.array(labels)

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)


class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
class_weight_dict = dict(enumerate(class_weights))


datagen = ImageDataGenerator(rotation_range=10, zoom_range=0.1, horizontal_flip=True)
datagen.fit(X_train)


base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=Input(shape=(image_size, image_size, 3)))
base_model.trainable = False
x = GlobalAveragePooling2D()(base_model.output)
x = Dense(64, activation='relu')(x)
output = Dense(1, activation='sigmoid')(x)

model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])
model.summary()


history = model.fit(datagen.flow(X_train, y_train, batch_size=8),
                    validation_data=(X_val, y_val),
                    epochs=10,
                    class_weight=class_weight_dict)


plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.legend(); plt.title('Accuracy'); plt.show()

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend(); plt.title('Loss'); plt.show()

y_pred = (model.predict(X_val) > 0.5).astype(int)
print(classification_report(y_val, y_pred))
cm = confusion_matrix(y_val, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Paper', 'Paper'], yticklabels=['Not Paper', 'Paper'])
plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Confusion Matrix'); plt.show()


from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode
from PIL import Image
import io

def take_photo(filename='photo.jpg', quality=0.8):
    js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      div.appendChild(video);

      document.body.appendChild(div);
      const stream = await navigator.mediaDevices.getUserMedia({video: true});
      video.srcObject = stream;
      await video.play();
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getTracks().forEach(track => track.stop());
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
    display(js)
    data = eval_js('takePhoto({})'.format(quality))
    binary = b64decode(data.split(',')[1])
    with open(filename, 'wb') as f:
        f.write(binary)
    return filename


image_path = take_photo()
img = load_img(image_path, target_size=(256, 256))
img_array = img_to_array(img) / 255.0
img_array = np.expand_dims(img_array, axis=0)

prediction = model.predict(img_array)[0][0]
label = "Paper" if prediction > 0.5 else "Not Paper"
print(f"Prediction: {label} ({prediction:.2f} confidence)")

from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode
from PIL import Image
import io

def take_photo(filename='photo.jpg', quality=0.8):
    js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      div.appendChild(video);

      document.body.appendChild(div);
      const stream = await navigator.mediaDevices.getUserMedia({video: true});
      video.srcObject = stream;
      await video.play();
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getTracks().forEach(track => track.stop());
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
    display(js)
    data = eval_js('takePhoto({})'.format(quality))
    binary = b64decode(data.split(',')[1])
    with open(filename, 'wb') as f:
        f.write(binary)
    return filename


image_path = take_photo()
img = load_img(image_path, target_size=(256, 256))
img_array = img_to_array(img) / 255.0
img_array = np.expand_dims(img_array, axis=0)

prediction = model.predict(img_array)[0][0]
label = "Paper" if prediction > 0.5 else "Not Paper"
print(f"Prediction: {label} ({prediction:.2f} confidence)")

from google.colab import files
from tensorflow.keras.preprocessing.image import load_img

def predict_upload():

    uploaded = files.upload()

    if not uploaded:
        print("No file uploaded! Please try again.")
        return


    for file_name in uploaded:
        try:

            img = load_img(file_name, target_size=(256, 256))
            img_array = img_to_array(img) / 255.0
            img_array = np.expand_dims(img_array, axis=0)


            prediction = model.predict(img_array)[0][0]
            confidence = prediction if prediction > 0.5 else 1 - prediction
            label = "PAPER" if prediction > 0.5 else "NOT PAPER"


            plt.figure(figsize=(6,6))
            plt.imshow(img)
            plt.title(f"{label}\nConfidence: {confidence:.2%}")
            plt.axis('off')
            plt.show()

        except Exception as e:
            print(f"Error processing {file_name}: {str(e)}")
            continue


predict_upload()